{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkJpOm22oJku",
        "outputId": "05cfbeaa-0944-46f8-e69a-44b4f1063614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.175.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.6.15)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai\n",
        "!pip install -q google-generativeai pypdf pdf2image pillow python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Meb8d_QpCvp",
        "outputId": "1ed131e3-fdd5-41a6-91c1-3485bf71a3f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n",
            "Fetched 186 kB in 1s (271 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126281 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for 1st page, page112"
      ],
      "metadata": {
        "id": "uXBBD4nhedG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# üîê Setup Gemini API\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from pdf2image import convert_from_path\n",
        "import re\n",
        "\n",
        "# ‚úÖ Load API key\n",
        "api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"‚ùå GOOGLE_API_KEY not found. Make sure it's stored in Colab secrets.\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# üéØ Initialize Gemini model\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# üìÑ Convert PDF to images\n",
        "def convert_pdf_to_images(pdf_path, output_folder, dpi=300):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    images = convert_from_path(pdf_path, dpi=dpi)\n",
        "    image_paths = []\n",
        "    for i, image in enumerate(images):\n",
        "        path = os.path.join(output_folder, f\"page_{len(image_paths)+1}.jpg\")\n",
        "        image.save(path, \"JPEG\")\n",
        "        image_paths.append(path)\n",
        "    return image_paths"
      ],
      "metadata": {
        "id": "L_CL0pxapWDY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract abbreviation dictionary from page 111\n",
        "abbrev_pdf_path = \"/content/1900_page111.pdf\"\n",
        "abbrev_img_paths = convert_pdf_to_images(abbrev_pdf_path, \"abbrev_imgs\")\n",
        "abbrev_image = Image.open(abbrev_img_paths[0])\n",
        "\n",
        "abbrev_prompt = \"\"\"\n",
        "You are reading a historical city directory page of abbreviations.\n",
        "Extract all abbreviations into a flat JSON dictionary like:\n",
        "\n",
        "{\n",
        "  \"acct\": \"accountant\",\n",
        "  \"adv\": \"advertisement\",\n",
        "  \"slsmn\": \"salesman\",\n",
        "  ...\n",
        "}\n",
        "\n",
        "- Keep punctuation/case in abbreviations (e.g., \"h.\", \"res.\")\n",
        "- Split forms like \"e or E\" into two separate keys\n",
        "- Only return valid abbreviation dictionary (no extra formatting)\n",
        "\"\"\"\n",
        "\n",
        "abbrev_response = model.generate_content([abbrev_prompt, abbrev_image])\n",
        "abbrev_text = abbrev_response.text.strip()\n",
        "if abbrev_text.startswith(\"```json\"):\n",
        "    abbrev_text = abbrev_text[7:].strip(\"` \\n\")\n",
        "abbrev_dict = json.loads(abbrev_text)\n",
        "\n",
        "# Save abbreviation dictionary\n",
        "with open(\"abbreviations.json\", \"w\") as f:\n",
        "    json.dump(abbrev_dict, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Extracted {len(abbrev_dict)} abbreviations.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "3LsbvHOjpUsv",
        "outputId": "fa3f0da3-eef3-4525-c34f-381e5a22f5da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extracted 98 abbreviations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# üîç Load abbreviations from page 111\n",
        "def extract_abbreviations(pdf_path):\n",
        "    image_path = convert_pdf_to_images(pdf_path, \"abbrev_imgs\")[0]\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    prompt = \"\"\"\n",
        "    Extract all abbreviations and full forms as a JSON dictionary. Format:\n",
        "    {\n",
        "      \"h.\": \"home\",\n",
        "      \"res.\": \"residence\",\n",
        "      \"slsmn\": \"salesman\"\n",
        "    }\n",
        "    Split forms like \"e or E - East\" as two keys. Do not return markdown or explanation.\n",
        "    \"\"\"\n",
        "    response = model.generate_content([prompt, image])\n",
        "    text = response.text.strip()\n",
        "    if text.startswith(\"```json\"):\n",
        "        text = text[7:].strip(\"` \\n\")\n",
        "    return json.loads(text)\n",
        "\n",
        "# üß† Gemini Prompt + Post-Processing Cleanup\n",
        "def split_image_vertically(image_path, parts=3):\n",
        "    image = Image.open(image_path)\n",
        "    width, height = image.size\n",
        "    segment_height = height // parts\n",
        "    return [image.crop((0, i * segment_height, width, (i + 1) * segment_height)) for i in range(parts)]\n",
        "\n",
        "def expand_abbreviations(text, abbr_dict):\n",
        "    if not text:\n",
        "        return text\n",
        "    words = text.split()\n",
        "    return \" \".join([abbr_dict.get(word.strip(\".,\").lower(), word) for word in words])\n",
        "\n",
        "def clean_company_name(name):\n",
        "    if not name:\n",
        "        return None\n",
        "    # Remove trailing business descriptions\n",
        "    exclusions = [\"full line of\", \"canned goods\", \"manufacturers of\", \"and bakers\", \"grocers\"]\n",
        "    name_clean = name.lower()\n",
        "    for excl in exclusions:\n",
        "        if excl in name_clean:\n",
        "            name_clean = name_clean[:name_clean.find(excl)]\n",
        "            break\n",
        "    return name_clean.strip(\", \").title()\n",
        "\n",
        "def extract_structured_info(image_paths, abbrev_dict, directory_name=\"Minneapolis 1900\", page_number=104, parts=3):\n",
        "    all_entries = []\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        slices = split_image_vertically(image_path, parts=parts)\n",
        "\n",
        "        for idx, img_slice in enumerate(slices):\n",
        "            prompt = f\"\"\"\n",
        "You are analyzing a 1900 Minneapolis city directory page column. Extract each entry as JSON using this format:\n",
        "\n",
        "{{\n",
        "  \"FirstName\": \"Peter D\",\n",
        "  \"LastName\": \"Aadland\",\n",
        "  \"Spouse\": \"Pearl R\",\n",
        "  \"Occupation\": \"Salesman\",\n",
        "  \"CompanyName\": \"Lifetime Sls\",\n",
        "  \"HomeAddress\": {{\n",
        "    \"StreetNumber\": \"2103\",\n",
        "    \"StreetName\": \"Bryant av S\",\n",
        "    \"ApartmentOrUnit\": \"apt 1\",\n",
        "    \"ResidenceIndicator\": \"h\"\n",
        "  }},\n",
        "  \"WorkAddress\": null,\n",
        "  \"Telephone\": null,\n",
        "  \"DirectoryName\": \"{directory_name}\",\n",
        "  \"PageNumber\": {page_number}\n",
        "}}\n",
        "\n",
        "Guidelines:\n",
        "- Do NOT carry forward last names between entries. If no last name is visible, use null.\n",
        "- Allow middle initials in first name like \"Anna B.\"\n",
        "- Do not include phrases like \"Full Line of...\" in company names.\n",
        "- Output a clean JSON array.\n",
        "\"\"\"\n",
        "            try:\n",
        "                response = model.generate_content([prompt, img_slice])\n",
        "                raw_output = response.text.strip()\n",
        "                if raw_output.startswith(\"```json\"):\n",
        "                    raw_output = raw_output[7:].strip(\"` \\n\")\n",
        "                parsed = json.loads(raw_output)\n",
        "\n",
        "                last_valid_lastname = None  # track independently for reset logic\n",
        "\n",
        "                for entry in parsed:\n",
        "                    # ‚úÖ Expand abbreviations\n",
        "                    if entry.get(\"Occupation\"):\n",
        "                        entry[\"Occupation\"] = expand_abbreviations(entry[\"Occupation\"], abbrev_dict)\n",
        "                    if entry.get(\"CompanyName\"):\n",
        "                        entry[\"CompanyName\"] = expand_abbreviations(clean_company_name(entry[\"CompanyName\"]), abbrev_dict)\n",
        "                    if entry.get(\"HomeAddress\") and entry[\"HomeAddress\"].get(\"StreetName\"):\n",
        "                        entry[\"HomeAddress\"][\"StreetName\"] = expand_abbreviations(entry[\"HomeAddress\"][\"StreetName\"], abbrev_dict)\n",
        "\n",
        "                    # ‚úÖ FIX: Remove carried-over last names\n",
        "                    fname = entry.get(\"FirstName\", \"\")\n",
        "                    lname = entry.get(\"LastName\", \"\")\n",
        "                    if lname and lname == last_valid_lastname:\n",
        "                        # Heuristics: likely a carryover if first name has middle initial and no new last name found\n",
        "                        if re.match(r'^[A-Z][a-z]+\\s+[A-Z]\\.?$', fname):  # e.g., \"Oscar S.\" or \"Anna B\"\n",
        "                            entry[\"LastName\"] = None\n",
        "                    elif lname and re.match(r\"^[A-Z][a-zA-Z]+$\", lname):  # Accept valid proper last name\n",
        "                        last_valid_lastname = lname  # Update tracker\n",
        "\n",
        "                all_entries.extend(parsed)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error in slice {idx}: {e}\")\n",
        "\n",
        "    return all_entries\n"
      ],
      "metadata": {
        "id": "GKr7ZUUloLU8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run on 1900_page112.pdf\n",
        "structured_pdf_path = \"/content/1900_page112.pdf\"\n",
        "output_dir = \"sliced_output\"\n",
        "image_paths = convert_pdf_to_images(structured_pdf_path, output_dir)\n",
        "\n",
        "# Extract and expand entries\n",
        "entries = extract_structured_info(image_paths, abbrev_dict, directory_name=\"Minneapolis 1900\", page_number=104)\n",
        "\n",
        "# Save final structured entries\n",
        "with open(\"structured_directory_output1.json\", \"w\") as f:\n",
        "    json.dump(entries, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Done. Extracted {len(entries)} structured entries and expanded abbreviations.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "aUnaR3Hfob_a",
        "outputId": "23040f12-509b-4441-f447-106f2fba0a8e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Done. Extracted 56 structured entries and expanded abbreviations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ue4aK9isux2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code For all 5 pages"
      ],
      "metadata": {
        "id": "EJJa8i4ReVHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîê Setup Gemini API\n",
        "!pip install -q google-generativeai\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from pdf2image import convert_from_path\n",
        "import re\n",
        "\n",
        "# ‚úÖ Load API key\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"‚ùå GOOGLE_API_KEY not found. Make sure it's stored in Colab secrets.\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# üéØ Initialize Gemini model\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# üìÑ Convert PDF to images\n",
        "def convert_pdf_to_images(pdf_path, output_folder, dpi=300):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    images = convert_from_path(pdf_path, dpi=dpi)\n",
        "    image_paths = []\n",
        "    for i, image in enumerate(images):\n",
        "        path = os.path.join(output_folder, f\"{os.path.splitext(os.path.basename(pdf_path))[0]}_page_{i+1}.jpg\")\n",
        "        image.save(path, \"JPEG\")\n",
        "        image_paths.append(path)\n",
        "    return image_paths\n",
        "\n",
        "# üß† Abbreviation extraction (ONCE, from page 111)\n",
        "def extract_abbreviations(pdf_path):\n",
        "    image_path = convert_pdf_to_images(pdf_path, \"abbrev_imgs\")[0]\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    prompt = \"\"\"\n",
        "    Extract all abbreviations and full forms as a JSON dictionary. Format:\n",
        "    {\n",
        "      \"h.\": \"home\",\n",
        "      \"res.\": \"residence\",\n",
        "      \"slsmn\": \"salesman\"\n",
        "    }\n",
        "    Split forms like \"e or E - East\" as two keys. Do not return markdown or explanation.\n",
        "    \"\"\"\n",
        "    response = model.generate_content([prompt, image])\n",
        "    text = response.text.strip()\n",
        "    if text.startswith(\"```json\"):\n",
        "        text = text[7:].strip(\"` \\n\")\n",
        "    return json.loads(text)\n",
        "\n",
        "def split_image_vertically(image_path, parts=3):\n",
        "    image = Image.open(image_path)\n",
        "    width, height = image.size\n",
        "    segment_height = height // parts\n",
        "    return [image.crop((0, i * segment_height, width, (i + 1) * segment_height)) for i in range(parts)]\n",
        "\n",
        "def expand_abbreviations(text, abbr_dict):\n",
        "    if not text:\n",
        "        return text\n",
        "    words = text.split()\n",
        "    return \" \".join([abbr_dict.get(word.strip(\".,\").lower(), word) for word in words])\n",
        "\n",
        "def clean_company_name(name):\n",
        "    if not name:\n",
        "        return None\n",
        "    exclusions = [\"full line of\", \"canned goods\", \"manufacturers of\", \"and bakers\", \"grocers\"]\n",
        "    name_clean = name.lower()\n",
        "    for excl in exclusions:\n",
        "        if excl in name_clean:\n",
        "            name_clean = name_clean[:name_clean.find(excl)]\n",
        "            break\n",
        "    return name_clean.strip(\", \").title()\n",
        "\n",
        "# üß† Main Extraction Function\n",
        "def extract_structured_info(image_paths, abbrev_dict, directory_name, page_number, parts=3):\n",
        "    all_entries = []\n",
        "    for image_path in image_paths:\n",
        "        slices = split_image_vertically(image_path, parts=parts)\n",
        "        for idx, img_slice in enumerate(slices):\n",
        "            prompt = f\"\"\"\n",
        "You are analyzing a 1900 Minneapolis city directory page column. Extract each entry as JSON using this format:\n",
        "\n",
        "{{\n",
        "  \"FirstName\": \"Peter D\",\n",
        "  \"LastName\": \"Aadland\",\n",
        "  \"Spouse\": \"Pearl R\",\n",
        "  \"Occupation\": \"Salesman\",\n",
        "  \"CompanyName\": \"Lifetime Sls\",\n",
        "  \"HomeAddress\": {{\n",
        "    \"StreetNumber\": \"2103\",\n",
        "    \"StreetName\": \"Bryant av S\",\n",
        "    \"ApartmentOrUnit\": \"apt 1\",\n",
        "    \"ResidenceIndicator\": \"h\"\n",
        "  }},\n",
        "  \"WorkAddress\": null,\n",
        "  \"Telephone\": null,\n",
        "  \"DirectoryName\": \"{directory_name}\",\n",
        "  \"PageNumber\": {page_number}\n",
        "}}\n",
        "\n",
        "Guidelines:\n",
        "- Do NOT carry forward last names between entries. If no last name is visible, use null.\n",
        "- Allow middle initials in first name like \"Anna B.\"\n",
        "- Do not include phrases like \"Full Line of...\" in company names.\n",
        "- Output a clean JSON array.\n",
        "\"\"\"\n",
        "            try:\n",
        "                response = model.generate_content([prompt, img_slice])\n",
        "                raw_output = response.text.strip()\n",
        "                if raw_output.startswith(\"```json\"):\n",
        "                    raw_output = raw_output[7:].strip(\"` \\n\")\n",
        "                parsed = json.loads(raw_output)\n",
        "\n",
        "                last_valid_lastname = None\n",
        "\n",
        "                for entry in parsed:\n",
        "                    if entry.get(\"Occupation\"):\n",
        "                        entry[\"Occupation\"] = expand_abbreviations(entry[\"Occupation\"], abbrev_dict)\n",
        "                    if entry.get(\"CompanyName\"):\n",
        "                        entry[\"CompanyName\"] = expand_abbreviations(clean_company_name(entry[\"CompanyName\"]), abbrev_dict)\n",
        "                    if entry.get(\"HomeAddress\") and entry[\"HomeAddress\"].get(\"StreetName\"):\n",
        "                        entry[\"HomeAddress\"][\"StreetName\"] = expand_abbreviations(entry[\"HomeAddress\"][\"StreetName\"], abbrev_dict)\n",
        "\n",
        "                    fname = entry.get(\"FirstName\", \"\")\n",
        "                    lname = entry.get(\"LastName\", \"\")\n",
        "                    if lname and lname == last_valid_lastname:\n",
        "                        if re.match(r'^[A-Z][a-z]+\\s+[A-Z]\\.?$', fname):\n",
        "                            entry[\"LastName\"] = None\n",
        "                    elif lname and re.match(r\"^[A-Z][a-zA-Z]+$\", lname):\n",
        "                        last_valid_lastname = lname\n",
        "\n",
        "                all_entries.extend(parsed)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error in slice {idx}: {e}\")\n",
        "    return all_entries\n",
        "\n",
        "# üìÅ Run full pipeline on pages 112‚Äì116\n",
        "pdf_files = [\n",
        "    \"/content/1900_page112.pdf\",\n",
        "    \"/content/1900_page113.pdf\",\n",
        "    \"/content/1900_page114.pdf\",\n",
        "    \"/content/1900_page115.pdf\",\n",
        "    \"/content/1900_page116.pdf\"\n",
        "]\n",
        "\n",
        "# ‚úÖ Step 1: Extract abbreviations ONCE from page 111\n",
        "abbrev_dict = extract_abbreviations(\"/content/1900_page111.pdf\")\n",
        "# Save abbreviation dictionary\n",
        "with open(\"abbreviations.json\", \"w\") as f:\n",
        "    json.dump(abbrev_dict, f, indent=2)\n",
        "print(f\"‚úÖ Extracted {len(abbrev_dict)} abbreviations.\")\n",
        "\n",
        "# ‚úÖ Step 2: Process all PDFs and combine results\n",
        "final_entries = []\n",
        "for i, pdf_path in enumerate(pdf_files, start=112):\n",
        "    print(f\"üîç Processing {pdf_path}\")\n",
        "    image_paths = convert_pdf_to_images(pdf_path, f\"images_page{i}\")\n",
        "    entries = extract_structured_info(image_paths, abbrev_dict, directory_name=\"Minneapolis 1900\", page_number=i)\n",
        "    final_entries.extend(entries)\n",
        "\n",
        "# ‚úÖ Step 3: Save final output\n",
        "with open(\"structured_directory_output.json\", \"w\") as f:\n",
        "    json.dump(final_entries, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Done. Extracted {len(final_entries)} total structured entries.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "cBjhyDEmsu0S",
        "outputId": "3c586ce2-ed01-4308-cd9f-b561b0843fec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extracted 98 abbreviations.\n",
            "üîç Processing /content/1900_page112.pdf\n",
            "üîç Processing /content/1900_page113.pdf\n",
            "üîç Processing /content/1900_page114.pdf\n",
            "üîç Processing /content/1900_page115.pdf\n",
            "‚ùå Error in slice 2: expected string or bytes-like object, got 'NoneType'\n",
            "üîç Processing /content/1900_page116.pdf\n",
            "‚úÖ Done. Extracted 334 total structured entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zf1WSXVQs4Wg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}